{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Technical University of Munich<br>\n",
    "Professorship of Environmental Sensing and Modeling<br><br>*\n",
    "**Author:**  Ali Ahmad Khan<br>\n",
    "**Date:**  13.11.2023\n",
    "\n",
    "--- \n",
    "\n",
    "# BAST Counting Data processing\n",
    "\n",
    "This script loads the 'BAST_CountingStations_daily.csv' files, cleans the dataset and converts the data into a predetermined data model. Only the sensors present in 'bast_locations_selected.gpkg' are worked upon<br>\n",
    "\n",
    "**Required steps**\n",
    "- Import file and convert columns to meaningful datatypes\n",
    "- Delete meaningsless columns and rows for detectors not included in locations\n",
    "- Convert the format from wide to long table and pivot to achieve the data model\n",
    "- Merge Counting Data with location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# import custom modules\n",
    "sys.path.append('../../utils/')\n",
    "import data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Clean raw data from the *.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder for BAST data\n",
    "data_path = data_paths.BAST_COUNTING_PATH\n",
    "\n",
    "# read bast location geopackaged file\n",
    "bast_loc = gpd.read_file(data_path+'bast_locations_selected.gpkg')\n",
    "\n",
    "# Reads Bast Counting data from the csv file\n",
    "bast_raw = pd.read_csv(data_path+'BAST_CountingStations_daily_new.csv', \n",
    "                       delimiter=',', decimal = ';', encoding='ISO-8859-1', index_col =0)\n",
    "\n",
    "# converts the time into datetime format YYYY-MM-DD\n",
    "bast_raw['date'] = pd.to_datetime(bast_raw['Datum'],format='%y%m%d')\n",
    "\n",
    "# Remove all columns that start with K_\n",
    "bast_raw = bast_raw[bast_raw.columns[~bast_raw.columns.str.startswith('K_')]]\n",
    "\n",
    "# Remove the named unnecessary column\n",
    "bast_raw = bast_raw.drop(['TKNR','Datum', 'Land', 'Strklas', 'Strnum', 'Wotag', 'Fahrtzw','PLZ_R1','PLZ_R2','Lkw_R1','Lkw_R2'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transormation\n",
    "\n",
    "### Create DataFrame with rows for each detector id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows only with Zst that are present in geopackage dataframe\n",
    "bast_raw = bast_raw.loc[bast_raw['Zst'].isin(bast_loc['MST_ID'].unique())]\n",
    "\n",
    "# Filter out the columns ending in R2 for df_r1 and vice versa for df_r2\n",
    "df_r1 = bast_raw[bast_raw.columns[~bast_raw.columns.str.endswith('R2')]].copy()\n",
    "df_r2 = bast_raw[bast_raw.columns[~bast_raw.columns.str.endswith('R1')]].copy()\n",
    "\n",
    "# Add suffix 1 and 2 to 'Zst' column for each DataFrame to create detector id\n",
    "df_r1['detector_id'] = df_r1['Zst'].astype(str) + '1'\n",
    "df_r2['detector_id'] = df_r2['Zst'].astype(str) + '2'\n",
    "\n",
    "# Remove the '_R1' suffix from df_r1 columns\n",
    "df_r1.columns = df_r1.columns.str.replace('_R1$', '', regex=True)\n",
    "\n",
    "# Remove the '_R2' suffix from df_r2 columns\n",
    "df_r2.columns = df_r2.columns.str.replace('_R2$', '', regex=True)\n",
    "\n",
    "# concatenate the dataframes with r1 and r2 columns\n",
    "transformed_bast_df = pd.concat([df_r1,df_r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe with per hour column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id VAriable to be used for melt\n",
    "id_vars = ['Zst','date','detector_id','Stunde']\n",
    "\n",
    "# Melting the dataframe to have vehicle types and their counts\n",
    "melted_bast_df = transformed_bast_df.melt(id_vars= id_vars,\n",
    "                    value_vars= transformed_bast_df.columns.difference(id_vars),\n",
    "                    var_name='vehicle_class',\n",
    "                    value_name='vehicle_count')\n",
    "\n",
    "# Pivot the table in regards to Stunde column\n",
    "pivoted_bast_df = pd.pivot_table( \n",
    "                            melted_bast_df,\n",
    "                            values='vehicle_count',\n",
    "                            index=['Zst','date', 'detector_id','vehicle_class'],\n",
    "                            columns=['Stunde'],\n",
    "                            aggfunc=\"sum\"\n",
    "                            ).reset_index().rename_axis(None, axis=1)\n",
    "\n",
    "# Dict to convert BAST vehicle classes to predetermined classes \n",
    "vehicle_class = {\n",
    "                 'KFZ': 'SUM',\n",
    "                 'Pkw': 'PC',\n",
    "                 'PmA': 'PC',\n",
    "                 'Lfw': 'LCV',\n",
    "                 'Lzg': 'HGV',\n",
    "                 'LoA': 'HGV',\n",
    "                 'Bus': 'BUS',\n",
    "                 'Mot': 'MOT',\n",
    "                }\n",
    "\n",
    "# Map the classes\n",
    "pivoted_bast_df['vehicle_class'] = pivoted_bast_df['vehicle_class'].map(vehicle_class)\n",
    "\n",
    "# FIll na and also assignd int to column\n",
    "pivoted_bast_df['detector_id'] = pivoted_bast_df['detector_id'].fillna(0).astype(int)\n",
    "\n",
    "# Assings all columns as float type for all stunde\n",
    "for col in range(1, 25):\n",
    "    pivoted_bast_df[col] = pivoted_bast_df[col].fillna(0).astype(float)\n",
    "\n",
    "# Group by the data  and sum it all up for given vehicle classes\n",
    "pivoted_bast_df = pivoted_bast_df.groupby(['Zst', 'date', 'detector_id','vehicle_class'], as_index=False).sum()\n",
    "\n",
    "# Assign detector type\n",
    "pivoted_bast_df['detector_type'] = np.where(pivoted_bast_df['vehicle_class'].isna(), np.NaN, '8+1')\n",
    "\n",
    "# Create a daily_value column that sums up data for all hours\n",
    "pivoted_bast_df['daily_value'] = pivoted_bast_df.loc[:, 1:24].sum(axis=1)\n",
    "\n",
    "# Assign the metric with volume value\n",
    "pivoted_bast_df['metric'] = 'volume'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marge Dataframe with location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the processed bast data with bast location\n",
    "processed_bast_df = pivoted_bast_df.merge(bast_loc, how = 'left', left_on = 'detector_id', right_on = 'DETEKTOR_ID').copy()\n",
    "\n",
    "# Assign a predetermined order to the columns\n",
    "column_order = ['date', 'road_link_id', 'detector_id', 'detector_type', 'vehicle_class','metric', 'daily_value'] + [i for i in range(1, 25)]\n",
    "processed_bast_df = processed_bast_df[column_order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store as parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to string to parquet storage\n",
    "processed_bast_df.columns = processed_bast_df.columns.astype(str)\n",
    "\n",
    "# Store the dataframe as a parquet file\n",
    "processed_bast_df.to_parquet(data_path+'preprocessed_bast_counting_data.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
