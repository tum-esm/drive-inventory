{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "from itertools import chain\n",
    "\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import data_paths\n",
    "from traffic_counts import TrafficCounts\n",
    "from hbefa_hot_emissions import HbefaHotEmissions\n",
    "from excel_calendar import Calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visum model\n",
    "visum = gpd.read_file(data_paths.VISUM_FOLDER_PATH + 'visum_links.gpkg')\n",
    "visum_red = visum.drop(visum.loc[(visum['road_type'] == 'none') | (visum['dtv_SUM']==0)].index, axis = 0)\n",
    "\n",
    "visum_red.loc[visum_red['road_type']=='Access-residential', 'road_type'] = 'Local/Collector'\n",
    "visum_red.loc[visum_red['road_type']=='Distributor/Secondary', 'road_type'] = 'TrunkRoad/Primary-National'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "# initialize calendar\n",
    "cal = Calendar()\n",
    "\n",
    "# initialize traffic cycles\n",
    "cycles = TrafficCounts()\n",
    "\n",
    "# initialize HBEFA emission factors\n",
    "hbefa = HbefaHotEmissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dates for normweekdays in 2019\n",
    "dates = cal.get_calendar()\n",
    "normweekdays_2019 = dates[(dates['day_type'] == 1) &\n",
    "                          (dates['date'].between('2019-01-01','2019-12-31'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "road_type                   vehicle_class\n",
       "Access-residential          HGV              0.003515\n",
       "                            LCV              0.083814\n",
       "                            MOT              0.044763\n",
       "                            PC               0.876182\n",
       "Local/Collector             BUS              0.008581\n",
       "                            HGV              0.030315\n",
       "                            LCV              0.099994\n",
       "                            MOT              0.013732\n",
       "                            PC               0.847378\n",
       "Motorway-Nat                BUS              0.004513\n",
       "                            HGV              0.158198\n",
       "                            LCV              0.085594\n",
       "                            MOT              0.003374\n",
       "                            PC               0.748321\n",
       "TrunkRoad/Primary-City      BUS              0.002192\n",
       "                            HGV              0.050612\n",
       "                            LCV              0.096303\n",
       "                            MOT              0.006461\n",
       "                            PC               0.844433\n",
       "TrunkRoad/Primary-National  BUS              0.006330\n",
       "                            HGV              0.056442\n",
       "                            LCV              0.096777\n",
       "                            MOT              0.010264\n",
       "                            PC               0.830187\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate vehicle shares on average norm weekday in 2019\n",
    "# these values are used to calculate the vehicle share correction factors\n",
    "\n",
    "weekday_shares_2019 = cycles.vehicle_shares.loc[:,normweekdays_2019['date']].reset_index()\n",
    "weekday_shares_2019 = weekday_shares_2019.groupby(['road_type','vehicle_class'])[0].mean()\n",
    "weekday_shares_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "visum_red['hgv_corr'] = visum_red.apply(lambda row : row['delta_HGV'] / weekday_shares_2019.loc[row['road_type'],'HGV'], axis = 1)\n",
    "visum_red['lcv_corr'] = visum_red.apply(lambda row : row['delta_LCV'] / weekday_shares_2019.loc[row['road_type'],'LCV'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-05-02'\n",
    "\n",
    "diurnal_cycles = cycles.get_hourly_scaling_factors(date=date)#.to_dict()\n",
    "vehicle_shares = cycles.get_vehicle_share(date=date).to_dict()\n",
    "daily_scaling = cycles.get_daily_scaling_factors(date=date).to_dict()\n",
    "\n",
    "em_sum_dict = dict()\n",
    "\n",
    "for row in visum_red.to_dict('records'):\n",
    "\n",
    "    # get relevant information from the visum model\n",
    "    road_link_id = row['road_link_id']\n",
    "    dtv_visum = row['dtv_SUM']\n",
    "    road_type = row['road_type']\n",
    "    hour_capacity = row['hour_capacity']\n",
    "    visum_speed = row['speed']\n",
    "    visum_slope = row['SLOPE']\n",
    "    hgv_corr = row['hgv_corr']\n",
    "    lcv_corr = row['lcv_corr']\n",
    "    \n",
    "    # get vehicle shares from counting data\n",
    "    hgv_share = vehicle_shares['HGV'][road_type]\n",
    "    lcv_share = vehicle_shares['LCV'][road_type]\n",
    "    pc_share = vehicle_shares['PC'][road_type]\n",
    "    mot_share = vehicle_shares['MOT'][road_type]\n",
    "    bus_share = vehicle_shares['BUS'][road_type]\n",
    "    \n",
    "    # calculate vehicle share correction factor\n",
    "    k = (1- (hgv_corr * hgv_share)- (lcv_corr * lcv_share)) / (1 - hgv_share - lcv_share)\n",
    "    \n",
    "    # calculate vehicle counts and apply correction factor\n",
    "    dtv = dict()\n",
    "    dtv_day = dtv_visum * daily_scaling[road_type]\n",
    "    dtv.update({'HGV' : (dtv_day * vehicle_shares['HGV'][road_type] * hgv_corr)})\n",
    "    dtv.update({'LCV' : (dtv_day * vehicle_shares['LCV'][road_type] * lcv_corr)})\n",
    "    dtv.update({'PC' : (dtv_day * vehicle_shares['PC'][road_type] * k)})\n",
    "    dtv.update({'MOT' : (dtv_day * vehicle_shares['MOT'][road_type] * k)})\n",
    "    dtv.update({'BUS' : (dtv_day * vehicle_shares['BUS'][road_type] * k)})\n",
    "    \n",
    "    # prepare array with vehicle counts in the same order as in the diurnal cylces\n",
    "    # dataframe. This allows to calculate hourly traffic volumes using \n",
    "    # vector multiplication\n",
    "    em = hbefa.calculate_emissions_daily(dtv, diurnal_cycles, road_type,\n",
    "                                        visum_speed, visum_slope, hour_capacity,  2019)\n",
    "    \n",
    "    emission_list = list(chain.from_iterable([list(el.values()) for el in list(em.values())]))\n",
    "    sum_dict = dict(reduce(add, (map(Counter, emission_list))))\n",
    "    \n",
    "    # multiply emission with link lenght\n",
    "    #TODO\n",
    "    \n",
    "    # add daily emission dict (value) and road link id (key) to final emission dict\n",
    "    em_sum_dict.update({road_link_id:sum_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_sum = dict(reduce(add, (map(Counter, list(em_sum_dict.values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CO': 39748241.11610593,\n",
       " 'NOx': 56034801.73405598,\n",
       " 'PM': 879980.3046771509,\n",
       " 'CO2(rep)': 19622172918.890606,\n",
       " 'CO2(total)': 20624028100.99998,\n",
       " 'NO2': 10015918.625387514,\n",
       " 'CH4': 815325.5815994816,\n",
       " 'BC (exhaust)': 496311.2002348853,\n",
       " 'CO2e': 19887360994.39458}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
