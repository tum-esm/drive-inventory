{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Technical University of Munich<br>\n",
    "Professorship of Environmental Sensing and Modeling<br><br>*\n",
    "**Author:**  Daniel KÃ¼hbacher & Ali Ahmad Khan<br>\n",
    "**Date:**  29.11.2023\n",
    "\n",
    "--- \n",
    "\n",
    "# Combine preprocessed datasets\n",
    "\n",
    "This file is to construct a final counting data product that combines different counting data sources, cleanes and aggregates the data and finally adds further information from different sources. The final product can be used in the emission generator pipeline to calculate actual emissions.\n",
    "\n",
    "**Required steps**\n",
    "- import counting data, the traffic model and the calendar\n",
    "- filter for metric \"volume\"\n",
    "- aggregate count data by road links (A single road link could have several detectors on different lanes. These need to be aggregated)\n",
    "- add road type information from the traffic model and day type infromation from the calendar\n",
    "- Filter for valid rows: check consistency between sum of hourly values and the daily sum and only accept data with daily value > 1; drop ivalid rows\n",
    "- Check for the consistency with the visum model -> add flag \"valid\" if consistent\n",
    "- Flag as \"incomplete\" if there is no data for the timeframe of interest\n",
    "- Store as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# import custom modules\n",
    "sys.path.append('../../utils')\n",
    "from excel_calendar import Calendar\n",
    "import data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and prepare datasets\n",
    "- preprocessed BASt traffic counting data (only traffic volume)\n",
    "- preprocessed LHM traffic counting data (traffic volume and speed)\n",
    "- traffic model\n",
    "- calendar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "mst_file_path = data_paths.MST_COUNTING_PATH + 'preprocessed_lhm_counting_data_until2023.parquet'\n",
    "bast_file_path = data_paths.BAST_COUNTING_PATH + 'preprocessed_bast_counting_data.parquet'\n",
    "visum_file_path = data_paths.VISUM_FOLDER_PATH + 'visum_links.gpkg'\n",
    "\n",
    "# import and concatenate bast and mst counting data\n",
    "counting_data = pd.concat([pd.read_parquet(mst_file_path), \n",
    "                           pd.read_parquet(bast_file_path)], axis = 0)\n",
    "counting_data = counting_data[counting_data['date'] < '2024-01-01']\n",
    "# import visum links data\n",
    "visum_links = gpd.read_file(visum_file_path)\n",
    "\n",
    "# initialize calendar object\n",
    "cal_obj = Calendar()\n",
    "\n",
    "print(f'Number of imported rows in the counting data: {len(counting_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare traffic volume dataset\n",
    "First we will filter the data for traffic volume counts and aggregate the single detector values. Then further information is added from the visum model and the calendar.</br>\n",
    "Rows with \"none\" road types will be dropped since they are not assigned to any road link in the visum model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate counts for each road_link_id and reduce to volume dataset\n",
    "volume  = counting_data.groupby(['metric','road_link_id',\n",
    "                                 'vehicle_class', 'date']).sum(numeric_only = True).loc['volume']\n",
    "volume = volume.drop(['detector_id'], axis = 1)\n",
    "volume = volume.reset_index()\n",
    "\n",
    "# add road type information\n",
    "road_types = visum_links.set_index('road_link_id')['road_type'].to_dict()\n",
    "volume.insert(3,'road_type' , volume['road_link_id'].map(road_types))\n",
    "# drop rows with road_type = \"none\"\n",
    "volume = volume[volume['road_type'] != 'none']\n",
    "\n",
    "# add day type information\n",
    "dates = volume['date'].unique()\n",
    "day_types = {date:cal_obj.get_day_type_combined(date) for date in dates}\n",
    "volume.insert(4, 'day_type', volume['date'].map(day_types))\n",
    "\n",
    "# drop rows with NaN values\n",
    "volume = volume.dropna()\n",
    "\n",
    "print(f'Number of rows: {len(volume)}')\n",
    "print(pd.Series([road_types[i] for i in volume['road_link_id'].unique()]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for valid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of all hour values of the day needs to be consistent with the sum of the day -> error_bound e=5%\n",
    "e = 0.025\n",
    "\n",
    "# Filter rows based on the condition\n",
    "volume_processed = volume[\n",
    "    volume.iloc[:, 6:].sum(axis=1).between(\n",
    "        volume['daily_value'] * (1 - e),\n",
    "        volume['daily_value'] * (1 + e)\n",
    "    )].copy()\n",
    "\n",
    "# drop rows with daily_volume <1\n",
    "volume_processed = volume_processed[volume_processed['daily_value'] > 1]\n",
    "\n",
    "print(f'Remaining number of rows: {len(volume_processed)}')\n",
    "print(pd.Series([road_types[i] for i in volume_processed['road_link_id'].unique()]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new DataFrame to store the outliers\n",
    "volume_processed_outliers_df = pd.DataFrame(columns= volume_processed.columns)\n",
    "\n",
    "# Define the threshold for modified Z-Score outliers (adjust as needed)\n",
    "modified_z_score_threshold = 3.5\n",
    "\n",
    "def detect_outliers(group):\n",
    "    daily_values = group['daily_value']\n",
    "    median = np.median(daily_values)\n",
    "    mad = np.median(np.abs(daily_values - median))\n",
    "    modified_z_scores = np.abs(0.6745 * (daily_values - median) / mad)\n",
    "    group['is_outlier'] = modified_z_scores > modified_z_score_threshold\n",
    "    return group\n",
    "\n",
    "# Apply the outlier detection function to each group and concatenate the results\n",
    "volume_processed_outliers_df = volume_processed.groupby(['road_link_id', 'vehicle_class', 'day_type']).apply(detect_outliers)\n",
    "\n",
    "# Reset the index and rename axis to obtain a flat column hierarchy\n",
    "volume_processed_outliers_df = volume_processed_outliers_df.reset_index(drop=True)\n",
    "volume_processed = volume_processed_outliers_df[volume_processed_outliers_df.is_outlier==False].drop('is_outlier', axis =1)\n",
    "\n",
    "print(f'Outlier Number of rows: {len(volume_processed_outliers_df[volume_processed_outliers_df.is_outlier==True])}')\n",
    "print(f'Remaining number of rows: {len(volume_processed)}')\n",
    "print(pd.Series([road_types[i] for i in volume_processed_outliers_df[volume_processed_outliers_df.is_outlier==False]['road_link_id'].unique()]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency with VISUM model\n",
    "\n",
    "For 2019 data we can check if the average daily count for weekdays outside the vacation time agrees with the visum model. The SQV (scalable quality value) is used with an f-factor of 10000. This sqv value indicates how well the modeled and observed data agrees while >0.8 is a good fit and >0.6 an acceptable fit. To avoid big influence by outliers, the inter-quantile range mean was selected to aggregate the counting data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_mean(input, iqr_range = (2.5, 97.5)):\n",
    "    lower_bound = np.percentile(input, iqr_range[0])\n",
    "    upper_bound = np.percentile(input, iqr_range[1])\n",
    "    return np.mean(input[(input >= lower_bound) & (input <= upper_bound)])\n",
    "\n",
    "def calc_sqv(Observed, Model, f=10000):\n",
    "    # f = 10000 is the recommended factor for daily volumes \n",
    "    return 1/(1 + sqrt(pow( Model - Observed, 2) / (f * Observed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_2019 = volume_processed[(volume_processed['date'].between('2019-01-01', '2019-12-31')) &\n",
    "                            (volume_processed['day_type'] == 0)]\n",
    "mean_cnt_2019 = cnt_2019.groupby(['road_link_id', 'vehicle_class'])['daily_value'].apply(iqr_mean).reset_index()\n",
    "mean_cnt_2019 = mean_cnt_2019.pivot(columns = 'vehicle_class', index = 'road_link_id')\n",
    "\n",
    "#aggregate dtv for each road link of the visum model\n",
    "visum_links['dtv_PC'] = visum_links['dtv_SUM'] * visum_links['delta_PC']\n",
    "visum_links['dtv_LCV'] = visum_links['dtv_SUM'] * visum_links['delta_LCV']\n",
    "visum_links['dtv_HGV'] = visum_links['dtv_SUM'] * visum_links['delta_HGV']\n",
    "visum_grp = visum_links.groupby('road_link_id')[['dtv_SUM','dtv_PC','dtv_LCV', 'dtv_HGV']].sum()\n",
    "\n",
    "visum_validation = pd.concat([visum_grp, mean_cnt_2019], axis =1).dropna()\n",
    "\n",
    "# calculate sqv value for each vehicle class\n",
    "for vehicle_class in ['SUM', 'PC', 'LCV', 'HGV']:\n",
    "    visum_validation['sqv_' + vehicle_class] = visum_validation.apply(\n",
    "        lambda row: calc_sqv(row[('daily_value' , vehicle_class)], row['dtv_' + vehicle_class]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sqv_threshold = 0.6\n",
    "# add sqv value for each vehicle class to the counting dataset\n",
    "volume_processed.insert(4,'sqv', 0.0)\n",
    "\n",
    "for idx, row in volume_processed.iterrows():\n",
    "    vehicle_class = row['vehicle_class']\n",
    "    try: \n",
    "        volume_processed.at[idx, 'sqv'] = visum_validation.loc[row['road_link_id'], 'sqv_' + vehicle_class]\n",
    "    except KeyError:\n",
    "        volume_processed.at[idx, 'sqv'] = 0.0\n",
    "\n",
    "volume_processed.insert(4,'valid', volume_processed['sqv'].apply(lambda x: True if (x > valid_sqv_threshold) or (x==0.0) else False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag incomplete timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = '2019-01-01'\n",
    "time_end = '2022-12-31'\n",
    "completeness_thres = 0.8\n",
    "\n",
    "# find counting stations that cover\n",
    "df = volume_processed[(volume_processed['vehicle_class'] == 'SUM') &\n",
    "                (volume_processed['date'].between(time_start, time_end))]\n",
    "\n",
    "df = df.groupby('road_link_id')['daily_value'].count()\n",
    "complete = df/df.max()\n",
    "volume_processed.insert(5, 'completness', 0.0)\n",
    "\n",
    "# add complete column to the counting dataset\n",
    "for idx,row in volume_processed.iterrows():\n",
    "    try:\n",
    "        volume_processed.at[idx, 'completness'] = complete[row['road_link_id']]\n",
    "    except KeyError: \n",
    "        volume_processed.at[idx, 'completness'] = 0.0\n",
    "        \n",
    "volume_processed.insert(4,'complete', volume_processed['completness'].apply(lambda x: True if (x > completeness_thres) or (x==0.0) else False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = volume_processed[(volume_processed['vehicle_class']=='SUM')]\n",
    "print(f'Number of rows in the dataset : {len(df)}')\n",
    "pd.Series([road_types[i] for i in df['road_link_id'].unique()]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = volume_processed[(volume_processed['vehicle_class']=='SUM') & \n",
    "                      (volume_processed['complete'] > 0.8)]\n",
    "print(f'Number of rows in the dataset : {len(df)}')\n",
    "pd.Series([road_types[i] for i in df['road_link_id'].unique()]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = volume_processed[(volume_processed['vehicle_class']=='SUM') & \n",
    "                      (volume_processed['complete'] > 0.8) &\n",
    "                      (volume_processed['sqv'] > 0.8)]\n",
    "print(f'Number of rows in the dataset : {len(df)}')\n",
    "pd.Series([road_types[i] for i in df['road_link_id'].unique()]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Road Types\n",
    "Since Access-residential, and TrunkRoad/Primary-National only have < 5 counting stations, they will be merged to Distributor/Secondary road_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_processed.insert(7, 'scaling_road_type', volume_processed['road_type'])\n",
    "\n",
    "volume_processed.loc[volume_processed['scaling_road_type']=='Access-residential', 'scaling_road_type'] = 'Distributor/Secondary'\n",
    "volume_processed.loc[volume_processed['scaling_road_type']=='TrunkRoad/Primary-National', 'scaling_road_type'] = 'Distributor/Secondary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store as Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to mst counting data\n",
    "data_path = data_paths.COUNTING_PATH\n",
    "\n",
    "# Store the dataframe as a parquet file\n",
    "volume_processed.to_parquet(data_path+'counting_data_combined_until2023.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
