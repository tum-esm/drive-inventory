{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Technical University of Munich<br>\n",
    "Professorship of Environmental Sensing and Modeling<br><br>*\n",
    "**Author:**  Daniel KÃ¼hbacher<br>\n",
    "**Date:**  06.02.2024\n",
    "\n",
    "--- \n",
    "\n",
    "# Calculate hot vehicle emissions using HBEFA emission factors\n",
    "\n",
    "<!--Notebook description and usage information-->\n",
    "This notebook implements the <utls/hot_emission_process.py> function and multiprocessing to calculate hot vehicle emissions for a given area. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import multiprocessing\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import data_paths\n",
    "from traffic_counts import TrafficCounts\n",
    "from hbefa_hot_emissions import HbefaHotEmissions\n",
    "from hot_emission_process import process_daily_emissions, process_hourly_emissions\n",
    "\n",
    "# Reload local modules on changes\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end time for emission calculation. Ideally this should cover a whole year.\n",
    "year = 2019\n",
    "start_date = datetime(year, 1, 1)\n",
    "end_date = datetime(year, 1, 10)\n",
    "\n",
    "# define filename of the visum file\n",
    "visum_filename = \"visum_links.GPKG\"\n",
    "\n",
    "# if True, the script will only calculate the emission for the area within the roi polygon\n",
    "clip_to_area = True\n",
    "roi_polygon = data_paths.MUNICH_BOARDERS_FILE # defines ROI for clipping\n",
    "\n",
    "# select aggregated or los-specific mode for traffic situation calculation\n",
    "mode = 'aggregated' \n",
    "#mode = 'los_specific'\n",
    "\n",
    "vehicle_classes = ['PC', 'LCV', 'HGV', 'BUS', 'MOT']\n",
    "components = ['NH3', 'CO', 'NOx', 'PM', 'PN', 'CO2(rep)',\n",
    "              'CO2(total)', 'NO2', 'CH4', 'NMHC',\n",
    "              'PM (non-exhaust)', 'Benzene', 'PM2.5', 'BC (exhaust)',\n",
    "              'PM2.5 (non-exhaust)', 'BC (non-exhaust)']\n",
    "components = ['CO2(total)']\n",
    "\n",
    "# Choose emission type: Tank-To-Wheel, Well-To-Tank (WTT), Well-To-Wheel (WTW)\n",
    "# WTW includes upstream emisssions from fuel production and distribution\n",
    "emission_type = 'EFA_weighted'\n",
    "#emission_type = 'EFA_WTT_weighted'\n",
    "#emission_type = 'EFA_WTW_weighted'\n",
    "\n",
    "# if True, the timeprofiles for the selected components will be calculated\n",
    "calculate_timeprofile = True\n",
    "store_timeprofiles = False\n",
    "\n",
    "# define number of processes for multiprocessing\n",
    "NUMBER_OF_PROCESSES = 7\n",
    "\n",
    "###\n",
    "#\n",
    "# STORE RESULTS\n",
    "#\n",
    "###\n",
    "\n",
    "store_results = False\n",
    "store_filename = f'linesource_Munich_2019_WTW_los_specific.gpkg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Initialize Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded emission factors from /Users/daniel_tum/Documents/code/traffic inventory v2/traffic-emission-inventory/data/restricted_input/hbefa/EFA_HOT_ts_hbefa.txt\n",
      "Loaded emission factors from /Users/daniel_tum/Documents/code/traffic inventory v2/traffic-emission-inventory/data/restricted_input/hbefa/EFA_HOT_aggregated_hbefa.txt\n"
     ]
    }
   ],
   "source": [
    "# import visum model\n",
    "visum = gpd.read_file(data_paths.VISUM_FOLDER_PATH + visum_filename)\n",
    "\n",
    "if clip_to_area:\n",
    "    roi = gpd.read_file(roi_polygon).to_crs(visum.crs)\n",
    "    visum = gpd.clip(visum, roi)\n",
    "    visum = visum.explode(ignore_index=True) # convert multipolygons to polygons\n",
    "\n",
    "#visum = visum_links\n",
    "visum = visum.reset_index(drop = True).reset_index() # reset index for calculation\n",
    "\n",
    "# initialize traffic cycles\n",
    "cycles = TrafficCounts()\n",
    "# initialize HBEFA emission factors\n",
    "hbefa = HbefaHotEmissions(components= components, \n",
    "                          vehicle_classes= vehicle_classes, \n",
    "                          ef_type = emission_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Inventory\n",
    "Use multiprocessing to calculate the emission for each road link day by day. This process will take some time to be finished for the whole area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculating 2019-01-01\n",
      "Finished calculating 2019-01-02\n",
      "Finished calculating 2019-01-03\n",
      "Finished calculating 2019-01-04\n",
      "Finished calculating 2019-01-05\n",
      "Finished calculating 2019-01-06\n",
      "Finished calculating 2019-01-07\n",
      "Finished calculating 2019-01-08\n",
      "Finished calculating 2019-01-09\n",
      "Finished calculating 2019-01-10\n"
     ]
    }
   ],
   "source": [
    "# calculate emission for each day\n",
    "\n",
    "dates = [d.strftime(\"%Y-%m-%d\") for d in pd.date_range(start = start_date,\n",
    "                                                       end = end_date,\n",
    "                                                       freq = '1d')]\n",
    "\n",
    "with multiprocessing.Manager() as manager: \n",
    "    \n",
    "    result_queue = manager.Queue()\n",
    "    error_queue = manager.Queue()\n",
    "    \n",
    "    with multiprocessing.Pool(NUMBER_OF_PROCESSES) as pool:\n",
    "        parameters = [(d,\n",
    "                       mode,\n",
    "                       visum.to_dict('records'),cycles,\n",
    "                       hbefa,\n",
    "                       result_queue,\n",
    "                       error_queue,\n",
    "                       ) for d in dates]\n",
    "        \n",
    "        res = pool.starmap(process_daily_emissions, parameters)\n",
    "    \n",
    "    # concatenate final process results.\n",
    "    result = result_queue.get() #get first result from queue\n",
    "    while not result_queue.empty():\n",
    "        print('Concatenate final process results')\n",
    "        new_result = result_queue.get()\n",
    "        for road_index, emissions in result.items():\n",
    "            for component, value in emissions.items():\n",
    "                add_emissions = new_result[road_index][component]\n",
    "                result[road_index][component] += add_emissions\n",
    "                \n",
    "    # retrieve process errors\n",
    "    errors = list()\n",
    "    while not error_queue.empty(): \n",
    "        errors.append(error_queue.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print errors\n",
    "for e in errors:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Results\n",
    "All results are saved in result dict. This can be appended to the traffic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate results and append to visum dataframe\n",
    "\n",
    "result_df = pd.DataFrame(result).transpose()\n",
    "result_df.columns = result_df.columns.map('_'.join)\n",
    "visum_result = pd.concat([visum, result_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if store_results is True\n",
    "\n",
    "if store_results: \n",
    "    \n",
    "    path = data_paths.INVENTORY_PATH\n",
    "    visum_result.to_file(path + store_filename, driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Save Timeprofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished day 2019-01-01\n",
      "finished day 2019-01-02\n",
      "finished day 2019-01-03\n",
      "finished day 2019-01-04\n",
      "finished day 2019-01-05\n",
      "finished day 2019-01-06\n",
      "finished day 2019-01-07\n",
      "finished day 2019-01-08\n",
      "finished day 2019-01-09\n",
      "finished day 2019-01-10\n"
     ]
    }
   ],
   "source": [
    "# only if store_result = True\n",
    "\n",
    "if calculate_timeprofile: \n",
    "    \n",
    "    # timeframe of interest\n",
    "    dates = [d.strftime(\"%Y-%m-%d\") for d in pd.date_range(start = start_date,\n",
    "                                                        end = end_date,\n",
    "                                                        freq = '1d')]\n",
    "\n",
    "    #placeholder for raw temporal profile\n",
    "    raw_profile = pd.DataFrame()\n",
    "    for day in dates:\n",
    "        em_dict = process_hourly_emissions(day,\n",
    "                                        visum[visum['road_type'] != 'Access-residential'].to_dict('records'), # reduce complexity\n",
    "                                        cycles,\n",
    "                                        hbefa)\n",
    "\n",
    "        em_sum = pd.DataFrame(em_dict).sum(axis = 1).reset_index()\n",
    "        em_sum.columns = ['vehcat', 'component', 'hour', 'emission']\n",
    "        em_fin = em_sum.groupby(['component', 'hour']).sum(numeric_only=True).reset_index()\n",
    "        em_fin['date'] = day\n",
    "        raw_profile = pd.concat([raw_profile, em_fin], axis = 0)\n",
    "        print('finished day', day)\n",
    "\n",
    "    # add timestamp and year to raw profile     \n",
    "    raw_profile['timestamp'] = pd.to_datetime(raw_profile['date'] + ' ' + raw_profile['hour'].astype(str) + ':00:00')\n",
    "    raw_profile['year'] = raw_profile['timestamp'].dt.year\n",
    "\n",
    "    # convert raw profile into scaling factors by dividing by mean emission\n",
    "    temporal_profile = pd.DataFrame()\n",
    "    for idx, grp in raw_profile.groupby(['component', 'year']):\n",
    "        grp['scaling_factor'] = grp['emission'] / grp['emission'].mean()\n",
    "        temporal_profile = pd.concat([temporal_profile, grp[['year', 'component', 'timestamp', 'scaling_factor']]], axis = 0)\n",
    "        \n",
    "    # store temporal profiles\n",
    "    if store_timeprofiles: \n",
    "        store_path = data_paths.INVENTORY_FOLDER_PATH +'/temporal_profiles/'\n",
    "\n",
    "        # store individual file for each year\n",
    "        for (year, component), data in temporal_profile.groupby(['year', 'component']):\n",
    "            temporal_profile.to_csv(store_path + f'temporal_profile_{component}_{year}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
