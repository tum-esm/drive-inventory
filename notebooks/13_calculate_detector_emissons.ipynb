{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Technical University of Munich<br>\n",
    "Professorship of Environmental Sensing and Modeling<br><br>*\n",
    "**Author:**  Daniel KÃ¼hbacher<br>\n",
    "**Date:**  07.05.2024\n",
    "\n",
    "--- \n",
    "\n",
    "# Detector Emission Calculation\n",
    "\n",
    "<!--Notebook description and usage information-->\n",
    "Calculates the traffic volume and the emissions based on the true counting data. This information is used in the uncertainty analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime, time\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import data_paths\n",
    "from hbefa_hot_emissions import HbefaHotEmissions\n",
    "\n",
    "# Reload local modules on changes\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year of investigation\n",
    "year = 2019\n",
    "start_date = datetime(year, 1, 1)\n",
    "end_date = datetime(year, 12, 31)\n",
    "\n",
    "# Define VISUM filename\n",
    "visum_filename = \"visum_links.GPKG\"\n",
    "\n",
    "#Define Counting Data filename\n",
    "cnt_data_filename  = 'counting_data_combined.parquet'\n",
    "\n",
    "# Define vehicle classes and components\n",
    "vehicle_classes = ['PC', 'LCV', 'HGV', 'BUS', 'MOT']\n",
    "components = ['CO2(rep)', 'CO2(total)', 'NOx', 'CO']\n",
    "\n",
    "###\n",
    "#\n",
    "# Save Data as parquet file\n",
    "#\n",
    "##\n",
    "\n",
    "save_results = False\n",
    "save_filepath = data_paths.INVENTORY_PATH + 'DetectorEmissions_2019_vc_estimate.feather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate emissions based on counting data\n",
    "\n",
    "def calculate_emissions(year: int,\n",
    "                        vehicle_class: str,\n",
    "                        vehicle_volume: int,\n",
    "                        TraSit: str,\n",
    "                        hbefa_gradient: str,\n",
    "                        component: str, \n",
    "                        hbefa_class: HbefaHotEmissions) -> float:\n",
    "    \"\"\"Calculates the emissions for a single vehicle class based on the given traffic volume and hbefa Traffic Situation\n",
    "\n",
    "    Args:\n",
    "        year (int): Year of investigation\n",
    "        vehicle_class (str): Vehicle Class\n",
    "        vehicle_volume (int): Traffic volume of the respective vehicle class\n",
    "        TraSit (str): Traffic situation \n",
    "        hbefa_gradient (str): Road gradient\n",
    "        component (str): Emission component\n",
    "        hbefa_class (HbefaHotEmissions): pre-initilized HBEFA object\n",
    "\n",
    "    Returns:\n",
    "        float: Emission estimate for the given input parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ef = hbefa_class.ef_dict['EFA_weighted'][year, \n",
    "                                                 TraSit,\n",
    "                                                 vehicle_class, \n",
    "                                                 hbefa_gradient,\n",
    "                                                 component]\n",
    "        \n",
    "    except KeyError:\n",
    "        ef = hbefa_class.ef_dict['EFA_weighted'][year, \n",
    "                                                 TraSit,\n",
    "                                                 vehicle_class, \n",
    "                                                 '0%',\n",
    "                                                 component]\n",
    "    return vehicle_volume * ef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visum and couting data and initialize hbefa class\n",
    "\n",
    "# import visum file\n",
    "visum_links = gpd.read_file(data_paths.VISUM_FOLDER_PATH + visum_filename)\n",
    "\n",
    "# import counting data\n",
    "cnt_data = pd.read_parquet(data_paths.COUNTING_PATH + cnt_data_filename)\n",
    "\n",
    "# subselect counting data for links that are in the visum network\n",
    "cnt_data = cnt_data[cnt_data['road_link_id'].isin(visum_links['road_link_id'].unique())]\n",
    "cnt_data = cnt_data[cnt_data['date'].between(start_date, end_date)].copy() # reduce to timeframe of interest\n",
    "\n",
    "# import hbefa emission module\n",
    "hbefa = HbefaHotEmissions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataframe with road link information for each detector location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare detector information dataframe\n",
    "\n",
    "road_info = pd.merge(cnt_data['road_link_id'].drop_duplicates(), visum_links[['road_type', 'hbefa_gradient',\n",
    "                                       'hbefa_speed', 'speed', 'road_link_id', 'hour_capacity']], \n",
    "                    left_on = 'road_link_id', \n",
    "                    right_on = 'road_link_id', \n",
    "                    how = 'inner')\n",
    "\n",
    "# set duplicates in road gradient to 0%\n",
    "road_info = road_info.groupby('road_link_id').agg({'speed': 'first', \n",
    "                                                'hbefa_speed': 'first',\n",
    "                                                'hour_capacity': 'sum',\n",
    "                                                'hbefa_gradient': lambda x: x.iloc[0] if len(x)==1 else '0%'}).reset_index()\n",
    "road_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare counting dataframe for emission calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare counting dataframe for emission calculation\n",
    "\n",
    "_cnt = cnt_data.melt(id_vars = ['date', 'road_link_id','road_type', 'vehicle_class'],\n",
    "                     value_vars = [str(x) for x in range(0,24)])\n",
    "\n",
    "_cnt['timestamp'] = _cnt.apply(lambda row: pd.Timestamp.combine(row['date'],\n",
    "                                                                time(int(row['variable']))), axis =1)\n",
    "\n",
    "_cnt = _cnt[_cnt['vehicle_class']!='SUM'] # delete 'SUM' vehicle class\n",
    "\n",
    "# prepare volume dataset\n",
    "_cnt_volume = _cnt.pivot(index = ['road_link_id', 'road_type', 'timestamp'],\n",
    "                                columns = 'vehicle_class',\n",
    "                                values = 'value')\n",
    "\n",
    "_cnt_volume['SUM_PCU'] = _cnt_volume.mul(pd.Series(hbefa.car_unit_factors)).sum(axis = 1)\n",
    "_cnt_volume = _cnt_volume.dropna().reset_index()\n",
    "\n",
    "cnt_volume = pd.merge(_cnt_volume, road_info, on = 'road_link_id', how = 'inner')\n",
    "\n",
    "# caclulate traffic condition\n",
    "cnt_volume['TraSit']= cnt_volume.apply(lambda row: hbefa.calc_los_class(hbefa_speed=row['hbefa_speed'],\n",
    "                                                                        hour_capacity=row['hour_capacity'],\n",
    "                                                                        htv_car_unit = row['SUM_PCU'],\n",
    "                                                                        road_type = row['road_type']),\n",
    "                                       axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate emissions for each detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate emissions for each component\n",
    "\n",
    "for c in components: \n",
    "    for vc in vehicle_classes:\n",
    "        cnt_volume[f'{vc}_{c}'] = cnt_volume.apply(lambda row: calculate_emissions(year = year,\n",
    "                                                                                   vehicle_class=vc,\n",
    "                                                                                   vehicle_volume= row[vc],\n",
    "                                                                                   TraSit=row['TraSit'],\n",
    "                                                                                   hbefa_gradient=row['hbefa_gradient'],\n",
    "                                                                                   component=c,\n",
    "                                                                                   hbefa_class=hbefa), axis = 1)\n",
    "cnt_volume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save detector emission results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if save_results is True\n",
    "if save_results: \n",
    "    cnt_volume.to_feather(save_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
