{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Technical University of Munich<br>\n",
    "Professorship of Environmental Sensing and Modeling<br><br>*\n",
    "**Author:**  Daniel KÃ¼hbacher<br>\n",
    "**Date:**  07.10.2024\n",
    "\n",
    "--- \n",
    "\n",
    "# Uncertainty of the single detector emission estimate\n",
    "\n",
    "<!--Notebook description and usage information-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "from datetime import time\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import data_paths\n",
    "from hbefa_hot_emissions import HbefaHotEmissions\n",
    "from hot_emission_process import process_hourly_emissions\n",
    "from traffic_counts import TrafficCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hbefa \n",
    "hbefa_obj = HbefaHotEmissions()\n",
    "# initialize traffic cycles\n",
    "cycles = TrafficCounts()\n",
    "# import visum data\n",
    "visum_links = gpd.read_file(data_paths.VISUM_FOLDER_PATH + 'visum_links.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import detector-based emissions for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import detector emission data\n",
    "_fname = data_paths.INVENTORY_FOLDER_PATH + 'DetectorEmissions_2019_vc_estimate.feather'\n",
    "#_fname = data_paths.INVENTORY_FOLDER_PATH + 'DetectorEmissions_2019.feather'\n",
    "df = pd.read_feather(_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Inventory Model to calculate the Emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_links_with_detectors = df['road_link_id'].unique()\n",
    "visum_reduced = visum_links[visum_links['road_link_id']\\\n",
    "    .isin(road_links_with_detectors)].copy()\n",
    "visum_reduced = visum_reduced.reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_emissions = list()\n",
    "\n",
    "for day in pd.date_range(start='2019-01-01', end= '2019-12-31', freq = 'd'): # for all days in 2019\n",
    "    \n",
    "    em_dict = process_hourly_emissions(day.strftime('%Y-%m-%d'),\n",
    "                                        visum_reduced.to_dict('records'),\n",
    "                                        cycles,\n",
    "                                        hbefa_obj)\n",
    "    \n",
    "    df_temp = pd.DataFrame()\n",
    "    for road_link_index, item in em_dict.items():\n",
    "        \n",
    "        data = pd.DataFrame(pd.Series(item).reset_index())\n",
    "        data.rename(columns = {'level_0':'vehicle_class',\n",
    "                               'level_1':'component',\n",
    "                               'level_2':'hour',\n",
    "                               0:'emission'},\n",
    "                    inplace = True)\n",
    "        data['date'] = day\n",
    "        data['road_link_index'] = road_link_index\n",
    "        df_temp = pd.concat([df_temp, data], axis = 0)\n",
    "    final_emissions.append(df_temp)\n",
    "model_dat = pd.concat(final_emissions)\n",
    "model_dat['road_link_id'] = model_dat['road_link_index'].map(visum_reduced['road_link_id'].to_dict())\n",
    "model_dat['timestamp'] = model_dat.apply(lambda row: pd.Timestamp.combine(row['date'],\n",
    "                                                                          time(row['hour'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize table\n",
    "model_dat_long = model_dat.groupby(['vehicle_class',\n",
    "                             'timestamp',\n",
    "                             'road_link_id',\n",
    "                             'component'])['emission'].sum().reset_index()\n",
    "\n",
    "model_dat_long = model_dat_long.pivot(index = ['road_link_id', 'timestamp'],\n",
    "                                      columns=['component', 'vehicle_class'],\n",
    "                                      values = 'emission')\n",
    "\n",
    "model_dat_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.DataFrame()\n",
    "for component in ['CO2(rep)', 'NOx', 'CO']:\n",
    "    \n",
    "    model = model_dat_long[component].sum(axis =1)\n",
    "    model.name = f'model_{component}'\n",
    "    detector = df.groupby(['road_link_id',\n",
    "                          'timestamp'])[[f'{x}_{component}' for x in hbefa_obj.vehicle_classes]].sum().sum(axis=1)\n",
    "    detector.name = f'detector_{component}'\n",
    "    \n",
    "    combined=pd.concat([combined, model, detector], axis = 1)\n",
    "    \n",
    "combined = combined.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate hourly differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(ax, mean, std, color=\"black\"):\n",
    "    x = np.linspace(mean-4*std, mean+4*std, 200)\n",
    "    p = stats.norm.pdf(x, mean, std)\n",
    "    z = ax.plot(x, p, color, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognormal(ax, s, loc, scale,  color=\"black\"):\n",
    "    x = np.linspace(-1, 1.5, 200)\n",
    "    p = stats.lognorm.pdf(x, s, loc = loc, scale = scale)\n",
    "    z = ax.plot(x, p, color, linewidth=2, linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize = (10, 6), tight_layout = True, sharey = True)\n",
    "\n",
    "col =0\n",
    "\n",
    "for component in ['CO2(rep)', 'NOx', 'CO']:\n",
    "    \n",
    "    df_h = combined[[f'model_{component}', f'detector_{component}']].copy()\n",
    "    df_d = df_h.reset_index(level = 0).groupby('level_0').resample('1d').sum()\n",
    "    df_a = df_h.reset_index(level = 0).groupby('level_0').resample('1Y').sum()\n",
    "    df_h['rel_diff'] = (df_h[f'model_{component}'] - df_h[f'detector_{component}'])/df_h[f'detector_{component}']\n",
    "    df_d['rel_diff'] = (df_d[f'model_{component}'] - df_d[f'detector_{component}'])/df_d[f'detector_{component}']\n",
    "    df_a['rel_diff'] = (df_a[f'model_{component}'] - df_a[f'detector_{component}'])/df_a[f'detector_{component}']\n",
    "    \n",
    "    val = df_h['rel_diff'][np.isfinite(df_h['rel_diff'].to_numpy())].to_numpy()\n",
    "    (lb, ub) = (np.percentile(val, 2.5), np.percentile(val, 97.5))\n",
    "    sub_val = val[(val>lb) & (val<ub)]\n",
    "    s, loc, scale = stats.lognorm.fit((sub_val+1), floc=0, fscale =1)\n",
    "    print(s, loc, scale)\n",
    "    lognormal(ax[0,col], s, loc-1, 1)\n",
    "    \n",
    "    sns.histplot(df_h, x='rel_diff',  ax = ax[0, col], binrange=(-1.5,1.5), bins= 30, stat=\"density\")\n",
    "    text = f'95% CI: [{lb*100:.1f}%, {ub*100:.1f}%] \\n$\\sigma$= {s*100:.1f}%'\n",
    "    ax[0,col].text(0.01, 0.78, text, transform=ax[0,col].transAxes)\n",
    "    ax[0,col].set_ylim(0, 1.8)\n",
    "\n",
    "    val1 = df_d['rel_diff'][np.isfinite(df_d['rel_diff'].to_numpy())].to_numpy()\n",
    "    (lb1, ub1) = (np.percentile(val1, 2.5), np.percentile(val1, 97.5))\n",
    "    sub_val1 = val1[(val1>lb1) & (val1<ub1)]\n",
    "    s, loc, scale = stats.lognorm.fit((sub_val1+1), floc=0, fscale =1)\n",
    "    print(s, loc, scale)\n",
    "    lognormal(ax[1,col], s, loc-1, 1)\n",
    "    sns.histplot(df_d, x='rel_diff',  ax = ax[1, col], binrange=(-1.5,1.5), bins= 30, stat=\"density\")\n",
    "    text = f'95% CI: [{lb1*100:.1f}%, {ub1*100:.1f}%] \\n$\\sigma$= {s*100:.1f}%'\n",
    "    ax[1,col].text(0.01, 0.78, text, transform=ax[1,col].transAxes)\n",
    "    ax[1,col].set_ylim(0, 1.8)\n",
    "    \n",
    "    val2 = df_a['rel_diff'][np.isfinite(df_a['rel_diff'].to_numpy())].to_numpy()\n",
    "    (lb2, ub2) = (np.percentile(val2, 2.5), np.percentile(val2, 97.5))\n",
    "    sub_val2 = val2[(val2>lb2) & (val2<ub2)]\n",
    "    s, loc, scale = stats.lognorm.fit((sub_val2+1), floc=0, fscale =1)\n",
    "    print(s, loc, scale)\n",
    "    lognormal(ax[2,col], s, loc-1, 1)\n",
    "    \n",
    "    sns.histplot(df_a, x='rel_diff',  ax = ax[2, col], binrange=(-1.5,1.5), bins= 30, stat=\"density\")\n",
    "    text = f'95% CI: [{lb2*100:.1f}%, {ub2*100:.1f}%] \\n$\\sigma$= {s*100:.1f}%'\n",
    "    ax[2,col].text(0.01, 0.78, text, transform=ax[2,col].transAxes)\n",
    "    ax[2,col].set_ylim(0, 2)\n",
    "    \n",
    "    col+=1\n",
    "    \n",
    "    ax[0,0].set_ylabel('Hourly Deviation', font = 'Helvetica', fontsize = 12)\n",
    "    ax[1,0].set_ylabel('Daily Deviation', font = 'Helvetica', fontsize = 12)\n",
    "    ax[2,0].set_ylabel('Annual Deviation', font = 'Helvetica', fontsize = 12)\n",
    "    \n",
    "    ax[0,0].set_title('CO2', font = 'Helvetica', fontsize = 12)\n",
    "    ax[0,1].set_title('NOx', font = 'Helvetica', fontsize = 12)\n",
    "    ax[0,2].set_title('CO', font = 'Helvetica', fontsize = 12)\n",
    "    \n",
    "    ax[2,0].set_xlabel(' ')\n",
    "    ax[2,2].set_xlabel(' ')\n",
    "    ax[2,1].set_xlabel('Relative difference', fontsize = 12, font = 'Helvetica')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Annual Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['rel_diff'] = (combined['model_CO2(rep)'] - combined['detector_CO2(rep)'])/combined['detector_CO2(rep)']\n",
    "lb_test = np.percentile(combined['rel_diff'], 2.5)\n",
    "ub_test = np.percentile(combined['rel_diff'], 97.5)\n",
    "\n",
    "combined_sub = combined[(combined['rel_diff']>lb_test) & (combined['rel_diff']<ub_test)]\n",
    "sns.scatterplot(data = combined, x='model_CO2(rep)', y='detector_CO2(rep)', s = 2, color = 'red')\n",
    "sns.scatterplot(data = combined_sub, x='model_CO2(rep)', y='detector_CO2(rep)', s = 10)\n",
    "plt.axline([0,0], [1,1], color = 'red', linestyle = '--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
